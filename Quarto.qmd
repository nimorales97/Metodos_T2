---
title: "Tarea 2"
subtitle: "Topicos Aplicados en Estadistica"
author: 
  - Nicolás Morales
  - Nicolás Bustos
format: pdf
code-block-bg: true
code-block-border-left: "#EBF3FA"
---

```{python}
#| echo: false
#| warning: false
import pandas as pd
import numpy  as np
import matplotlib.pyplot as plt
import seaborn as sns
import scipy
from IPython.display import Markdown
import math
```

## Pre-procesamiento de los datos

Para este trabajo lo primero es importar los datos:

```{python}
#| warning: false
#| label: Importar base de datos
Hitters = pd.read_csv("Hitters.csv")
Hitters = Hitters.rename(columns={"Unnamed: 0" : "Name"})
Hitters['Name'] = Hitters['Name'].apply(lambda x: x[1:])
```

A continuación se muestra una parte de las tres primeras filas de la tabla recién creada:

```{python}
#| warning: false
#| echo: false
print(Hitters.head(3).to_string(index = False, justify = 'center', max_cols = 8))
```

Acto seguido se realiza un conteo de valores faltantes (`NaN`). En la siguiente salida se aprecia que solo la columna *Salary* contiene estos eventos, con un total de 59 ocurrencias.

```{python}
#| warning: false
suma_nan = Hitters.isnull().sum(axis = 0)
print(suma_nan[suma_nan != 0])
```

Se realiza un filtro, redefiniendo la base de datos dejando sólo aquellas filas libres de valores faltantes.

```{python}
Hitters_NAN = Hitters[Hitters["Salary"].isnull()]
Hitters     = Hitters[Hitters["Salary"].isnull() == False]
```

Puesto que la distribución del salario es sesgada, se pide considerar en su lugar el logaritmo del salario:

```{python}
Hitters['Salary'] = Hitters['Salary'].apply(lambda x: math.log(x))
```

```{python}
#| eval: false
#| include: false

# plt.clf()
# plt.boxplot(Hitters["Salary"])
# plt.show()
```

Se verifica que las variables numéricas efectivamente tomen valores positivos de acuerdo a su construcción:

```{python}
#| warning: false
aux = Hitters[Hitters.columns.difference(['Name','League','Division','NewLeague'])] < 0
aux.sum(axis = 0)
```

Para las variables numéricas, se examinan los posibles *outliers*:

```{python}
#| echo: false
#| warning: false
#| code-overflow: wrap
def mi_outliers(serie):
  Q1,Q3 = np.percentile(serie , [25,75])
  IQR = Q3 - Q1
  U = Q3+1.5*IQR
  L = Q1-1.5*IQR
  filtro = serie[(serie > U) | (serie < L)]
  if (filtro.shape[0] != 0):
    return(" <- "+str(filtro.shape[0])+" outliers.")
  else:
    return("")
Hitters[Hitters.columns.difference(['Name','League', 'Division', 'NewLeague'])].apply(mi_outliers)
```

Escalar las variables para evitar problemas numéricos al ajustar la regresión:

```{python}
def mi_scale(serie):
  mean = sum(serie) / len(serie)
  standard_deviation = math.sqrt( sum( (serie - mean)**2 ) / len(serie))
  return((serie - mean) / standard_deviation)

Hitters[Hitters.columns.difference(['Name','League', 'Division', 'NewLeague'])] = Hitters[Hitters.columns.difference(['Name','League', 'Division', 'NewLeague'])].apply(lambda x: mi_scale(x))
```

Para el manejo de variables categóricas, se generan variables *dummy* definidas como indicadoras 0-1:

```{python}
Hitters = pd.get_dummies(Hitters, columns = ['League', 'Division', 'NewLeague'], drop_first = True)
```

## ¿Cuáles son las características más importantes para predecir el salario de los jugadores?

### Primer modelo: Regresión lasso

```{python}
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error
y=Hitters["Salary"]
X=Hitters.drop(["Salary","Name"], axis=1)
X_train, X_test, y_train, y_test = train_test_split(X, y, 
                                                    test_size=0.20, 
                                                   random_state=46)
from sklearn.linear_model import Ridge, Lasso
from sklearn.linear_model import RidgeCV, LassoCV

lasso_model = Lasso().fit(X_train, y_train)
y_pred=lasso_model.predict(X_test)
np.sqrt(mean_squared_error(y_test, y_pred))
```

```{python}
alphas = 10**np.linspace(10,-2,100)*0.5
lasso_cv_model = LassoCV(alphas = alphas, cv = 10).fit(X_train, y_train)
print(lasso_cv_model.alpha_)
```

```{python}
lasso_tuned = Lasso(alpha = lasso_cv_model.alpha_).fit(X_train, y_train)
y_pred = lasso_tuned.predict(X_test)
np.sqrt(mean_squared_error(y_pred,y_test))
```

```{python}
coeficientes = pd.DataFrame({'Names':X_train.columns.values.tolist(),'Coef':lasso_tuned.coef_.tolist()})
coeficientes[coeficientes["Coef"] != 0]
```

### Segundo modelo: Elastic Net

```{python}
from sklearn.linear_model import Ridge, Lasso, ElasticNet
from sklearn.linear_model import RidgeCV, LassoCV,ElasticNetCV
```

```{python}
enet_model = ElasticNet().fit(X_train, y_train)
y_pred = enet_model.predict(X_test)
np.sqrt(mean_squared_error(y_test, y_pred))
```

```{python}
alphas = 10**np.linspace(10,-2,100)*0.5
enet_cv_model = ElasticNetCV(alphas = alphas, cv = 10).fit(X_train, y_train)
enet_cv_model.alpha_
```

```{python}
enet_tuned = ElasticNet(alpha = enet_cv_model.alpha_).fit(X_train, y_train)
y_pred = enet_tuned.predict(X_test)
np.sqrt(mean_squared_error(y_test, y_pred))
```

```{python}
coeficientes = pd.DataFrame({'Names':X_train.columns.values.tolist(),'Coef':enet_tuned.coef_.tolist()})
coeficientes[coeficientes["Coef"] != 0]
```
